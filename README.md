# N-gram Language Modeling

This project focuses on building N-gram language models (unigram, bigram, trigram) to understand and analyze patterns in sequential text data.

## 🚀 Features

- Tokenized and cleaned raw text data
- Built unigram, bigram, and trigram models
- Calculated frequencies and probabilities
- Implemented sentence generation using N-gram chains
- Visualized common N-grams

## 🛠️ Technologies Used

- Python
- NLTK
- Matplotlib / Seaborn
- Pandas

## 📂 Project Structure

- `ngram_modeling.ipynb`: Main notebook with step-by-step implementation
- `data/`: Text datasets used for training

## 📊 Sample Results

- Most common bigrams: `('of', 'the'), ('in', 'the'), ('to', 'the')`
- Generated sentences using bigram model:
  > *"The data is important for machine learning tasks."*
