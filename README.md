# N-gram Language Modeling

This project focuses on building N-gram language models (unigram, bigram, trigram) to understand and analyze patterns in sequential text data.

## ðŸš€ Features

- Tokenized and cleaned raw text data
- Built unigram, bigram, and trigram models
- Calculated frequencies and probabilities
- Implemented sentence generation using N-gram chains
- Visualized common N-grams

## ðŸ› ï¸ Technologies Used

- Python
- NLTK
- Matplotlib / Seaborn
- Pandas

## ðŸ“‚ Project Structure

- `ngram_modeling.ipynb`: Main notebook with step-by-step implementation
- `data/`: Text datasets used for training

## ðŸ“Š Sample Results

- Most common bigrams: `('of', 'the'), ('in', 'the'), ('to', 'the')`
- Generated sentences using bigram model:
  > *"The data is important for machine learning tasks."*
